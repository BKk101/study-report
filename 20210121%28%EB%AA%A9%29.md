1. 학습 날짜 // 

    2021-01-21(목)
 
2. 학습시간 // 

    09:00 ~ 18:00 (자가)
    
3. 학습 범위 및 주제 // 
    
    파이썬, 머신러닝
    
4. 동료 학습 방법 (온라인 또는 오프라인으로 동료 학습이 이뤄진 경우, 어떤 식으로 이뤄졌는지 기술) // 오프라인으로 만난경우 각각 슬랙 아이디 // 온라인(슬랙, 스카이프, 카카오톡)으로 피드백을 받는 경우 피드백해준 자 ID // 

    온라인을 통해 동료학습 하였다.  피드백해준 자 :gicho, jeonkim, sanhan, jungtlee, youcho, sanglee

5. 학습 목표 //

    머신러닝의 기본 개념을 학습한다.
    
6. 과제 제출 repository 주소 // 
    
    
    
7. 상세 학습 내용 (운영진 판단하에 학습 내용이 부진할 경우 재 제출 요구 예정) 필수사항 이외에 자유롭게 작성 가능 실제 코딩에 사용한 시간 (필수) 학습에 참고한 사이트 (홈페이지, 블로그, 동영상, 자료)와 간단한 내용(권고) 오늘 발견한 문제(버그, 몰랐던 것,...)와 해결 방안(필수) 다른 교육생들과의 협업, 토론 내용(권고) //
    
    09:00 ~ 18:00 동안 코드를 작성하였다.
        
        x_input = np.array([[1, 1], [2, 2.5], [2.5, 1.3], [4.3, 9.5], [5.5, 7.0], [6, 8.2], [7, 5], [8, 6], [9, 4.5]], dtype=np.float32)
        labels = np.array([[1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1], [0, 0, 1]], dtype=np.float32)

        n_var, n_class = 2, 3
        W = np.random.normal(size=(n_var, n_class))
        B = np.random.normal(size=(1,n_class))

        def Softmax(y):
            c = np.max(y, axis=1)
            c = c.reshape((-1, 1))
            exp_y = np.exp(y - c)

            sum_exp_y = np.sum(exp_y, axis=1)
            sum_exp_y = sum_exp_y.reshape((-1, 1))

            res_y = exp_y / sum_exp_y
            return res_y

        def Hypothesis(x):
          return Softmax(np.matmul(x, W) + B)

        # Cost Funciton: Cross Entropy Error
        def Cost():  
          return np.mean(-np.sum(labels * np.log(Hypothesis(x_input)),axis=1))

        # 기울기(Gradient) : 수치 미분
        def Gradient():
            global W, B
            pres_w = W.copy()
            grad_w = np.zeros_like(W)
            pres_b = B.copy()
            grad_b = np.zeros_like(B)
            delta = 1e-7

            for idx_r in range(W.shape[0]):
                for idx_c in range(W.shape[1]):
                    W[idx_r, idx_c] = pres_w[idx_r, idx_c] + delta
                    cost_p = Cost()
                    W[idx_r, idx_c] = pres_w[idx_r, idx_c] - delta
                    cost_m = Cost()
                    grad_w[idx_r, idx_c] = (cost_p-cost_m)/(2*delta)
                    W[idx_r, idx_c] = pres_w[idx_r, idx_c]

            for idx in range(B.size):
                B[0][idx] = pres_b[0][idx] + delta
                cost_p = Cost()
                B[0][idx] = pres_b[0][idx] - delta
                cost_m = Cost()
                grad_b[0][idx] = (cost_p-cost_m)/(2*delta)
                B[0][idx] = pres_b[0][idx]

            return grad_w, grad_b

        # 학습 (Training)
        epochs = 50000
        learning_rate = 0.01
        training_idx = np.arange(0, epochs+1, 1)
        cost_graph = np.zeros(epochs+1)

        for cnt in range(0, epochs+1):
          cost_graph[cnt] = Cost()
          if cnt % (epochs/20) == 0:
            print("[{:>5}] cost = {:>10.4}".format(cnt, cost_graph[cnt]))

          grad_w, grad_b = Gradient()
          W -= learning_rate * grad_w
          B -= learning_rate * grad_b
  

8. 학습 내용에 대한 개인적인 총평 (최소 5줄 이상) //
    
    머신러닝을 이용한 분류에서 0과 1 두가지 경우가 아닌 두개 이상의 경우로 분류를 할 수 있다. 이 경우 0,1 두가지로 분류하는 방식과는 다른 방식을 통해 알고리즘을 구현해야 한다. sigmoid 를 이용한 방식으로는 0,1 두가지로만 분류할 수 있기 때문에 Softmax를 이용하여 전체 중에서 각 등급의 해당하는 확률을 계산하여 등급을 분류한다. softmax 방법에서 그래프를 y-c 만큼 평행이동 하여 값이 0~1 사이로만 나오게 하고 발산하는 것을 방지하여 학습률을 높일 수 있다.

    
9. 다음 학습 계획 (최소 5줄 이상) // 
    
    자료구조, 파이썬에 대해 학습한다.
    
    libasm에 대해 공부하고 평가를 받는다.
    
    c++언어를 복습하며 심화학습한다.