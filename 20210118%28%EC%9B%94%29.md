1. 학습 날짜 // 

    2021-01-18(월)
 
2. 학습시간 // 

    09:00 ~ 18:00 (자가)
    
3. 학습 범위 및 주제 // 
    
    파이썬, 머신러닝
    
4. 동료 학습 방법 (온라인 또는 오프라인으로 동료 학습이 이뤄진 경우, 어떤 식으로 이뤄졌는지 기술) // 오프라인으로 만난경우 각각 슬랙 아이디 // 온라인(슬랙, 스카이프, 카카오톡)으로 피드백을 받는 경우 피드백해준 자 ID // 

    온라인을 통해 동료학습 하였다.  피드백해준 자 :gicho, jeonkim, sanhan, jungtlee, youcho, sanglee

5. 학습 목표 //

    머신러닝의 기본 개념을 학습한다.
    
6. 과제 제출 repository 주소 // 
    
    
    
7. 상세 학습 내용 (운영진 판단하에 학습 내용이 부진할 경우 재 제출 요구 예정) 필수사항 이외에 자유롭게 작성 가능 실제 코딩에 사용한 시간 (필수) 학습에 참고한 사이트 (홈페이지, 블로그, 동영상, 자료)와 간단한 내용(권고) 오늘 발견한 문제(버그, 몰랐던 것,...)와 해결 방안(필수) 다른 교육생들과의 협업, 토론 내용(권고) //
    
    09:00 ~ 18:00 동안 코드를 작성하였다.

        # Input and Labels
        x_input = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype= np.float32)
        labels = np.array([3, 5, 7, 9, 11, 13, 15, 17, 19, 21], dtype= np.float32)

        # Weights
        W = np.random.normal()
        B = np.random.normal()

        # Hypothesis : Linear Function
        def Hypothesis(x):    
            return W*x + B

        # Cost : Mean Squared Error 
        def Cost():
            return np.mean((Hypothesis(x_input) - labels)**2)

        def Gradient(x, y):
            return np.mean(x*(x*W+(B-y))), np.mean((W*x-y+B))

        # def Gradient(x, y):
        #     global W, B
        #     pres_w, pres_b = W, B # W,B backup
        #     delta = 5e-7 

        #     W = pres_w + delta
        #     cost_p = Cost()
        #     W = pres_w - delta
        #     cost_m = Cost()
        #     grad_w = (cost_p-cost_m)/(2*delta)

        #     B = pres_b + delta
        #     cost_p = Cost()
        #     B = pres_b - delta
        #     cost_m = Cost()
        #     grad_b = (cost_p-cost_m)/(2*delta)

        #     W, B = pres_w, pres_b # w,b restore
        #     return grad_w, grad_b

        # Parameter Set
        epochs = 5000
        learning_rate = 0.005

        # 학습 (Training)
        for cnt in range(0, epochs+1):
            if cnt % (epochs/20) == 0:
              print("[{:>5}] cost = {:>10.4}, W = {:>7.4}, B = {:>7.4}".format(cnt, Cost(), W, B))

            grad_w, grad_b = Gradient(x_input, labels)
            W -= learning_rate * grad_w
            B -= learning_rate * grad_b
   
8. 학습 내용에 대한 개인적인 총평 (최소 5줄 이상) //
    
    머신러닝의 기본 개념에 대해 학습하였다. 머신 러닝의 가장 기본 원리는 여러개의 데이터를 가장 잘 표현하는 식을 구하는 것이고 이를 위해 선형회귀 등 여러가지 방법을 이용하는 것이다. 식의 기울기와 계수를 조금씩 수정하면서 실제 데이터와 얼마나 잘 맞는지를 비교하고 이 과정을 반복적으로 수행하여 차이를 최소화 하는 과정을 데이터를 통한 학습으로 부른다. 머신러닝은 이러한 조건을 사람이 미리 결정하는 것을 말하며 딥러닝은 이러한 조건 마저 컴퓨터 스스로 결정해서 학습하도록 하는 것을 말한다.
    
    머신러닝에서 정답으로 여겨지는 실제 데이터를 label이라 한다. 선형회귀에서는 데이터를 표현하는 식을 1차함수인 직선으로 가정하고 학습을 통해 기울기와 y절편 값을 수정하며 label과의 오차를 최소화한다. 이 과정에서 gradient descent 방법이 이용된다. 계산식을 이용하여 간단하게 표현하거나 직접 계산과정을 코드를 통해 구현할 수 있다.

    
9. 다음 학습 계획 (최소 5줄 이상) // 
    
    자료구조, 파이썬에 대해 학습한다.
    
    libasm에 대해 공부하고 평가를 받는다.
    
    c++언어를 복습하며 심화학습한다.