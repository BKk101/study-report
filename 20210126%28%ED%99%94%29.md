1. 학습 날짜 // 

    2021-01-26(화)
 
2. 학습시간 // 

    09:00 ~ 18:00 (자가)
    
3. 학습 범위 및 주제 // 
    
    파이썬, 머신러닝
    
4. 동료 학습 방법 (온라인 또는 오프라인으로 동료 학습이 이뤄진 경우, 어떤 식으로 이뤄졌는지 기술) // 오프라인으로 만난경우 각각 슬랙 아이디 // 온라인(슬랙, 스카이프, 카카오톡)으로 피드백을 받는 경우 피드백해준 자 ID // 

    온라인을 통해 동료학습 하였다.  피드백해준 자 :gicho, jeonkim, sanhan, jungtlee, youcho, sanglee

5. 학습 목표 //

    CNN에 대해 학습한다.
    
6. 과제 제출 repository 주소 // 
    
    
    
7. 상세 학습 내용 (운영진 판단하에 학습 내용이 부진할 경우 재 제출 요구 예정) 필수사항 이외에 자유롭게 작성 가능 실제 코딩에 사용한 시간 (필수) 학습에 참고한 사이트 (홈페이지, 블로그, 동영상, 자료)와 간단한 내용(권고) 오늘 발견한 문제(버그, 몰랐던 것,...)와 해결 방안(필수) 다른 교육생들과의 협업, 토론 내용(권고) //
    
    09:00 ~ 18:00 동안 코드를 작성하였다.

        # Logistic regression : Logic Gate Truth Table
        x_input = tf.constant([[0, 0], [0, 1], [1, 0], [1, 1]],dtype=tf.float32)
        labels = tf.constant([[0], [1], [1], [0]],dtype=tf.float32)  # Gate : XOR

        # Weight, Bias
        W0 = tf.Variable(tf.random.normal((2, 2), dtype=tf.float32))
        B0 = tf.Variable(tf.random.normal((1, 2), dtype=tf.float32))

        W1 = tf.Variable(tf.random.normal((2, 1), dtype=tf.float32))
        B1 = tf.Variable(tf.random.normal((1, 1), dtype=tf.float32)) 

        # Hyoithesis, Cost, Optimizer
        def Hypothesis(x):
          hidden = tf.sigmoid(tf.matmul(x, W0) + B0)  
          return tf.sigmoid(tf.matmul(hidden, W1) + B1)

        eps = 1e-7  # prevent log(0) => infinite
        def Cost():
          return -tf.reduce_mean(labels * tf.math.log(Hypothesis(x_input)+eps) + (1 - labels) * tf.math.log(1 - Hypothesis(x_input) + eps)) 

        epochs = 50000
        learning_rate = 0.1
        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)

        training_idx = np.arange(0, epochs+1, 1)
        cost_graph = np.zeros(epochs+1)
        check = np.array([0, epochs*0.01, epochs*0.1, epochs*0.4, epochs*0.6, epochs])
        w_trained = []
        b_trained = []
        check_idx = 0

        for cnt in range(0, epochs+1):
          cost_graph[cnt] = Cost()
          if cnt % (epochs/20) == 0:
            print("[{:>5}] cost = {:>10.4}".format(cnt, cost_graph[cnt]))
          if check[check_idx] == cnt:
            w_trained.append(W0.numpy())
            b_trained.append(B0.numpy())
            check_idx += 1

          optimizer.minimize(Cost, [W0, W1, B0, B1])

       
   
8. 학습 내용에 대한 개인적인 총평 (최소 5줄 이상) //

    MLP의 경우 perceptron을 하나 추가하기만 해도 연산 횟수가 많이 증가한다. 단순한 기능을 구현하는 것만해도 많은 수의 perceptron layer가 필요하기 때문에 초장기 이론이 등장했을 때에는 컴퓨터 성능의 한계로 MLP가 발전되지 못하였다.
    
    이후 backpropagation이라는 방법이 등장하며 gradient 계산시 연산횟수를 많이 줄일 수 있게 되었고 MLP에 대한 발전이 가속화 되었다. backpropagation을 이용하는 경우 특정 노드 이전에 있는 노드가 모두 같은 값을 활용하기 때문에 연산량을 많이 줄일 수 있다고 하였으나 완전히 이해하지는 못하였다.
    
9. 다음 학습 계획 (최소 5줄 이상) // 
    
    자료구조, 파이썬에 대해 학습한다.
    
    libasm에 대해 공부하고 평가를 받는다.
    
    c++언어를 복습하며 심화학습한다.